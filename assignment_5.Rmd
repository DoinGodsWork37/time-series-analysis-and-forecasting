---
title: 'Assignment 5: Beer Sales'
author: "Joshua Goldberg"
date: "`r format(Sys.time(), '%B, %d %Y')`"
output:
  pdf_document: default
  github_document: null
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: inline
always_allow_html: yes
---

```{r Global-options, include=FALSE}
knitr::opts_chunk$set(fig.width=6, fig.asp=0.618, fig.path='Figs/',
                      warning=FALSE, message=FALSE, cache=TRUE)
```

```{r Preamble, echo=FALSE}
# Enter package in p_load()
# If package is not installed, p_load() will install and load the package
if(!"pacman" %in% rownames(installed.packages())) {
  install.packages("pacman")
  }
pacman::p_load(tidyverse, ggthemes, here, tsibble, lubridate, tsibble, zoo,
               tseries, rlang, broom, forecast, janitor, lubridate, readxl,
               fpp, xts, TSA)

options(stringsAsFactors = FALSE)

# Set default ggplot theme to tufte
theme_set(ggthemes::theme_tufte())
```

Load data from TSA package (the package is written by authors Jonathan Cryer and Kung-Sik Chan).

```{r}
data(beersales)
```

The data is the monthly beer sales in millions of barrels, 01/1975 - 12/1990.

Train: 01/1975 - 12/1989.

Test: 1990

# Part 1

Use `ARIMA(p,d,q)` model to forecast beer sales for all months of 1990 using the following two multi-step forecasting approaches. For each model, check mean, autocorrelation and normality of the residuals. Confirm if the residuals are white noise.

```{r}
train <- window(beersales, c(1975, 1), c(1989, 12))
test <- window(beersales, c(1990, 1), c(1990, 12))
```

```{r}
arima_fit <- auto.arima(train)
summary(arima_fit)
```

## 1A 

Use the h-period in `forecast()` to forecast each month of 1990. This is also known as recursive forecasting where you fit a model only once and use it recursively for h-periods.

```{r}
ts.plot(arima_fit$residuals)
```

```{r}
acf(arima_fit$residuals)
```

```{r}
(recursive_forecast <- forecast(arima_fit, h = 12))
```

## 1B

Use the monthly data as a continuous time series. Forecast for 1990 Jan, Plug forecast into the time series, build a new model to forecast for 1990 Feb. And so on and so forth. In other words, h=1 in all the forecasts. This is known as direct recursive (DirRec) forecasting where you fit a new model for each time step.

```{r}
dir_rec <- function(.data, .model_count, refit = FALSE) {
  predictions <- vector("numeric", .model_count)
  models <- vector("list", .model_count)
  
  new_data <- train %>% as_tsibble(index = index) %>% append_row(.model_count)
  model_fit <- auto.arima(new_data %>% drop_na() %>% as_tsibble(index = index) %>% as.ts())
  orders <- arimaorder(model_fit)
  
  # Index used for appending new data
  index_change <- .model_count - 1
  
    for (i in 1:.model_count) {
      
      model_data <- new_data %>% drop_na() %>% as_tsibble(index = index) %>% as.ts()
      
      model <- Arima(model_data,
                     order = orders[c("p", "d", "q")],
                     seasonal = orders[c("P", "D", "Q")])
      
      forecast_point <- forecast(model, h = 1)$mean %>% as.numeric()
      
      predictions[[i]] <- forecast_point
      new_data[nrow(new_data) - index_change, "value"] <- forecast_point
      models[[i]] <- model
      
      # Reduce index by 1 floored to zero if negative
      index_change <- ifelse(index_change - 1 < 0, 0, index_change - 1)
      
      # Refit `auto.arima()` to obtain new parameters with prediction in consideration 
      if (refit)  {
        model_fit <- auto.arima(model_data, stepwise = FALSE)
        orders <- arimaorder(model_fit)
      }
    }
  
  list(data = new_data, models = models, predictions = predictions)
}

model_same_params <- dir_rec(train, 12, refit = FALSE)
model_refit <- dir_rec(train, 12, refit = TRUE)
```

## 1C

Plot the mean, the p-value of the autocorrelation test and the p-value of the normality test of the residuals of the 12 models. The Box test results fail to reject the null hypothesis (the data are independently distributed). The data visually do not look too bad, but we reject the null hypothesis (data is normally distributed) of the shapiro test.

```{r}
map_dbl(model_same_params$models, ~ Box.test(.x$residuals, lag = 24)$p.value) %>% plot()

map_dbl(model_same_params$models, ~ shapiro.test(.x$residuals)$p.value) %>% plot()

map(1:12, ~
data.frame(r = model_same_params$models[[.x]]$residuals %>% as.numeric()) %>% 
ggplot(aes(sample = r)) +
stat_qq(pch = 1) +
stat_qq_line() +
labs(subtitle = paste0("Model: ", .x))) %>%
patchwork::wrap_plots() +
patchwork::plot_annotation(title = "Residual Diagnostics: QQ Plot")

walk(model_same_params$models, ~ checkresiduals(.x))
```

# Part 2

Plot the Recursive and DirRec along with the actuals. Use ylim=c(12.5, 17) to get a good visual of the plot differences.

```{r}
test_predictions <- test %>%
  as_tsibble() %>% 
  mutate(direct_recursive = model_refit$predictions,
         recursive_model = recursive_forecast %>% as_tibble() %>% pull(`Point Forecast`))

test_predictions %>%
  rename(actual = value) %>% 
  gather(key = type, value = value) %>% 
  ggplot(aes(index, value, color = type)) +
  geom_point() +
  geom_line() +
  scale_color_viridis_d(name = NULL, labels = c("Actual", "Direct Recursive", "Recursive")) +
  labs(title = "Direct Recursive (refitting arima each run) vs. Recursive",
       x = "Date",
       y = "Sales")
```

# Part 3

Calculate the MSE for 1990 - which of the two approaches take larger computation time and why? Direct Recursive (DR) is better from . Computationally it is more intensive, since it has to refit the model with new data.

```{r}
mse_recursive = mean((test_predictions$recursive_model - test_predictions$value)^2)
mse_direct_recursive = mean((test_predictions$direct_recursive - test_predictions$value)^2)

glue::glue("MSE recursive: {mse_recursive}")
glue::glue("MSE direct recursive: {mse_direct_recursive}")
```




